{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNpPeoutlTRCOMwBB8PYr9F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsaha205/Spring_23_MLO/blob/main/HW_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1"
      ],
      "metadata": {
        "id": "hr3Av_u2RFOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$S_α(\\mathbf{x_0}) = argmin_{x \\in R^d}\\frac{1}{2α} \\|\\mathbf{x} - \\mathbf{x_0}\\|^2 + \\|x\\|_1$\n",
        "\n",
        "To show that the soft-shrinkage operator on vectors given by $S_α(x_0)$ is well defined, we need to show that the objective function is strictly convex.\n",
        "Now, the first term is a strictly convex function as it's hessian metrix is positive definite. And the second term is also convex since the L1-norm n is convex. Therefore, the sum of the two terms is stricly convex function that means the objective function is well difined and there is a unique minimizer.\n"
      ],
      "metadata": {
        "id": "Edpl5TKMc-dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, from the participation 4 for a scalar input a, we have,\n",
        "\n",
        "$s_α(a) = \\begin{cases}\n",
        "a - \\alpha, & \\text{if } a > \\alpha \\\\\n",
        "0, & \\text{if } -\\alpha \\leq a \\leq \\alpha \\\\\n",
        "a + \\alpha, & \\text{if } a < -\\alpha\n",
        "\\end{cases}$\n",
        "\n",
        "\n",
        "Now we have,\n",
        "\n",
        "$S_α(\\mathbf{x_0}) = argmin_{x \\in R^d}\\frac{1}{2α} \\|\\mathbf{x} - \\mathbf{x_0}\\|^2 + \\|x\\|_1 $\n",
        "\n",
        "$S_α(\\mathbf{x_0}) = argmin_{x \\in R^d}\\frac{1}{2α} \\sum_{i=1}^d (x_i - (x_0)_i)^2 + \\sum_{i=1}^d |x_i| $\n",
        "\n",
        "$S_α(\\mathbf{x_0}) = argmin_{x \\in R^d} \\sum_{i=1}^d \\frac{1}{2α}[(x_i - (x_0)_i)^2 + |x_i|] $ \n",
        "\n",
        "Now after seperating the individual component we can write,\n",
        "\n",
        "$S_α(\\mathbf{x_0})_i  = s_α((\\mathbf{x_0})_i)$ \n",
        "\n",
        "After simplyfying $s_α((\\mathbf{x_0})_i)$  we can write,\n",
        "\n",
        "$ s_α((\\mathbf{x_0})_i) = sign((\\mathbf{x_0})_i) max(|(\\mathbf{x_0})_i| -α, 0)$\n",
        "\n"
      ],
      "metadata": {
        "id": "1saKSsFdoArn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implementation of **softShrink(x0, alpha)** is given below."
      ],
      "metadata": {
        "id": "L2RRJlHb61gO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ITGUTkyKRAvn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softShrink(x0, alpha):\n",
        "    return np.sign(x0) * np.maximum(np.abs(x0) - alpha, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3"
      ],
      "metadata": {
        "id": "QwnBb9iRxkjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the LASSO problem that is,\n",
        "\n",
        "$\\mathbf{x}^* = argmin_{x \\in R^d}\\frac{1}{2} \\|\\mathbf{Ax} - b\\|_2^2 + λ\\|x\\|_1$\n",
        "\n",
        "Assume, $f(\\mathbf{x}) = \\frac{1}{2} \\|\\mathbf{Ax} - b\\|_2^2$ and $g(\\mathbf{x}) = λ\\|x\\|_1$,\n",
        "\n",
        "So we write the above LASSO problem in the form of $min_{\\mathbf{x}} f(\\mathbf{x})+ g(\\mathbf{x})$\n",
        "\n",
        "Now from the (Prox) of Problem-2 we can write that,\n",
        "\n",
        "$\\mathbf{x}_{t+1} = argmin_{x \\in R^d}\\frac{1}{2α_t} \\|\\mathbf{x} - (\\mathbf{x}_t - α_t\\nabla f(\\mathbf{x}_t) \\|_2^2 + g(\\mathbf{x})$\n",
        "\n",
        "$\\mathbf{x}_{t+1} = argmin_{x \\in R^d}\\frac{1}{2α_t} \\|\\mathbf{x} - (\\mathbf{x}_t - α_t \\mathbf{A}^T (\\mathbf{Ax_t} - b)\\|_2^2 +  λ\\|x\\|_1$\n",
        "\n",
        "$\\mathbf{x}_{t+1} = s_{αλ} (\\mathbf{x}_t - α_t \\mathbf{A}^T (\\mathbf{Ax_t} - b))$\n",
        "\n",
        "Where $s_{αλ}$ is the soft-shrinkage operator and defined as,\n",
        "\n",
        "$ s_α(\\mathbf{y}) = sign(\\mathbf{y}) * max(|\\mathbf{y}| - αλ, 0)$\n"
      ],
      "metadata": {
        "id": "26eoMCDC0ztx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implementation of the given function [xT, objhist] = istaLasso(A, b, lambda_reg, x0, alpha, T) is given below."
      ],
      "metadata": {
        "id": "CUQPla7l6wgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def istaLasso(A, b, lambda_reg, x0, alpha, T):\n",
        "    \n",
        "    # Initialize variables\n",
        "    x_t = x0\n",
        "    objhist = []\n",
        "\n",
        "    # appending the initial objective function\n",
        "    objhist.append(np.linalg.norm(A @ x_t - b) ** 2 / 2 + lambda_reg * np.linalg.norm(x_t, ord=1))\n",
        "    \n",
        "    for i in range(T):\n",
        "        # Compute y as mentioned\n",
        "        y = x_t - alpha (A.T @ (A @ x_t - b))\n",
        "        \n",
        "        # Update estimate of x\n",
        "        x_t = softShrink(y, alpha*lambda_reg)\n",
        "        \n",
        "        # Compute objective value at current estimate of x\n",
        "        objhist.append(np.linalg.norm(A @ x_t - b) ** 2 / 2 + lambda_reg * np.linalg.norm(x_t, ord=1))\n",
        "    \n",
        "    return x_t, objhist"
      ],
      "metadata": {
        "id": "BHq38FqdeSa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4"
      ],
      "metadata": {
        "id": "VWBY75cN_9bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LASSO problem is,\n",
        "\n",
        "$\\mathbf{x}^* = argmin_{x \\in R^d}\\frac{1}{2} \\|\\mathbf{Ax} - b\\|_2^2 + λ\\|\\mathbf{x}\\|_1$\n",
        "\n",
        "Now, the gradient of the first part is straight forward which is, $\\mathbf{A}^T (\\mathbf{Ax} - b)$.\n",
        "\n",
        "And the gradient of the second part can be witten as $λs$ where,\n",
        "\n",
        "$s_i = \\begin{cases} \\text{sign}(x_i), & x_i \\neq 0\\\\ [-1,1], & x_i = 0 \\end{cases}$\n",
        "\n"
      ],
      "metadata": {
        "id": "fXpIi2EqAAHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implementaion of the  g = lassoSubgrad(A, b, lambda_reg, x) function that returns a subgradient for the LASSO objective at x is given below."
      ],
      "metadata": {
        "id": "0zXvGqwABTBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lassoSubgrad(A, b, lambda_reg, x):\n",
        "    # compute s \n",
        "    s = np.zeros_like(x)\n",
        "    s[x > 0] = 1\n",
        "    s[x < 0] = -1\n",
        "    s[np.abs(x) <= 1e-8] = np.random.uniform(low=-1, high=1, size=s[np.abs(x) <= 1e-8].shape)\n",
        "    \n",
        "    return A.T @ (A @ x - b) + lambda_reg * s"
      ],
      "metadata": {
        "id": "85JvwdMPBc5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implementaion of the [xT, objhist] = subgradLasso(A, b, lambda_reg, x0, alpha, T) is given below."
      ],
      "metadata": {
        "id": "NTNn82EAB57h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def subgradLasso(A, b, lambda_reg, x0, alpha, T):\n",
        "    \n",
        "    # Initialize variables\n",
        "    x_t = x0\n",
        "    objhist = []\n",
        "\n",
        "    # appending the initial objective function\n",
        "    objhist.append(np.linalg.norm(A @ x_t - b) ** 2 / 2 + lambda_reg * np.linalg.norm(x_t, ord=1))\n",
        "    \n",
        "    for i in range(T):\n",
        "        # Compute the sugradient delta\n",
        "        delta = lassoSubgrad(A, b, lambda_reg, x_t)\n",
        "        \n",
        "        # Update estimate of x\n",
        "        x_t = x_t - alpha * delta\n",
        "        \n",
        "        # Compute objective value at current estimate of x\n",
        "        objhist.append(np.linalg.norm(A @ x_t - b) ** 2 / 2 + lambda_reg * np.linalg.norm(x_t, ord=1))\n",
        "    \n",
        "    return x_t, objhist"
      ],
      "metadata": {
        "id": "WLRHMvszCDBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5"
      ],
      "metadata": {
        "id": "eJGhj33VCJ8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JnxGjcimCNeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 6"
      ],
      "metadata": {
        "id": "fFJfn_dM3wdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 6(a)"
      ],
      "metadata": {
        "id": "WQ_PoYg13yEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If $\\mathbf{x^*}$ is a solution to $argmin_{\\mathbf{x}} f(\\mathbf{x})+  λ\\|\\mathbf{x}\\|_1$, from Fermat’s optimality condition we know,\n",
        "\n",
        "$0 \\in ∇ f(\\mathbf{x^*}) + λsign(\\mathbf{x^*})$ which we can write as,\n",
        "\n",
        "$ ∇ f(\\mathbf{x^*}) + λsign(\\mathbf{x^*}) = 0$\n",
        "\n",
        "==> $ ∇ f(\\mathbf{x^*}) = -λsign(\\mathbf{x^*})$\n",
        "\n",
        "==> $ \\|∇ f(\\mathbf{x^*})\\|_2^2 = λ^2\\|sign(\\mathbf{x^*})\\|_2^2 \\hspace{1in} $(by sqaring 2-norm in both sides)\n",
        "\n",
        "==> $ C^2 > λ^2\\|sign(\\mathbf{x^*})\\|_2^2 \\hspace{1in}$    (as $\\|∇ f(\\mathbf{x^*})\\|_2 < C$)\n",
        "\n",
        "==> $ C^2 > λ^2 n \\hspace{1in}$ (where $n$ is the number of non-zero entries in $\\mathbf{x^*}$)\n",
        "\n",
        "==> $ n <  \\frac{C^2}{λ^2}$\n",
        "\n",
        "Now from the above derivation of number of non-zerp entries in $\\mathbf{x^*}$, we can see that by increasing $λ$ while using $l1$ regularization, number of non-zero entries in $\\mathbf{x^*}$ will be decreased proportionally which ensures that $\\mathbf{x^*}$ is increasingly sparse."
      ],
      "metadata": {
        "id": "LgStMaxA3_vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 6(b)"
      ],
      "metadata": {
        "id": "k77LRemLApfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AYTIeKRqArKy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Lu8uCM531jN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}